{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textloader import Loader\n",
    "from preprocessing.utils import remove_empty_docs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets : Amazon Review and IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Amazon Product review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = Loader.load_amazon_reviews('train')\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very diverse dataset and we are taking a subset of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    100020\n",
       "0     99980\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_size = 200000\n",
    "dataset = train_df.sample(n=Sample_size, random_state=42)\n",
    "dataset.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000,) (200000,)\n"
     ]
    }
   ],
   "source": [
    "corpus_amazon = dataset['review'].values\n",
    "target_amazon = dataset['sentiment'].values\n",
    "print(corpus_amazon.shape, target_amazon.shape)\n",
    "corpus_amazon, target_amazon = remove_empty_docs(corpus_amazon, target_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading IMDB datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_imdb = Loader.load_imdb_data('train')\n",
    "test_df_imdb = Loader.load_imdb_data('test')\n",
    "corpus_imdb = train_df_imdb['review'].values\n",
    "target_imdb = train_df_imdb['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine IMDB and Amazon corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN COUPUS= 225000\n"
     ]
    }
   ],
   "source": [
    "corpus = np.concatenate((corpus_imdb , corpus_amazon))\n",
    "target = np.concatenate((target_imdb , target_amazon))\n",
    "print(\"LEN COUPUS=\",len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for Skip-Gram Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(doc.lower()) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#May take 2-3 minutes to run\n",
    "w2v_model = Word2Vec(tokenized_corpus,\n",
    "                     sg=1, #FOR SKIP-GRAM\n",
    "                     vector_size = 50,\n",
    "                     window = 5,\n",
    "                     min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulory Size: 54667\n"
     ]
    }
   ],
   "source": [
    "w2v_model.wv.save_word2vec_format(fname = 'word2vec.txt')\n",
    "word_vectors = KeyedVectors.load_word2vec_format('word2vec.txt', binary=False)\n",
    "vocab_size = len(word_vectors.index_to_key)\n",
    "print(\"Vocabulory Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>broken</th>\n",
       "      <td>cracked</td>\n",
       "      <td>crumbled</td>\n",
       "      <td>dented</td>\n",
       "      <td>damaged</td>\n",
       "      <td>chipped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damaged</th>\n",
       "      <td>dented</td>\n",
       "      <td>defective</td>\n",
       "      <td>broken</td>\n",
       "      <td>scratched</td>\n",
       "      <td>openned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>awsome</td>\n",
       "      <td>amazing</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>excelent</td>\n",
       "      <td>phenominal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>usefull</td>\n",
       "      <td>helpful</td>\n",
       "      <td>doable</td>\n",
       "      <td>valuable</td>\n",
       "      <td>practical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>great</td>\n",
       "      <td>decent</td>\n",
       "      <td>ok</td>\n",
       "      <td>nice</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easy</th>\n",
       "      <td>difficult</td>\n",
       "      <td>simple</td>\n",
       "      <td>quick</td>\n",
       "      <td>dificult</td>\n",
       "      <td>cinch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>violent</th>\n",
       "      <td>brutal</td>\n",
       "      <td>sadistic</td>\n",
       "      <td>cruel</td>\n",
       "      <td>disturbing</td>\n",
       "      <td>misogynistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>romantic</th>\n",
       "      <td>screwball</td>\n",
       "      <td>romance</td>\n",
       "      <td>comedy</td>\n",
       "      <td>fairytale</td>\n",
       "      <td>quirky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasty</th>\n",
       "      <td>gross</td>\n",
       "      <td>filthy</td>\n",
       "      <td>icky</td>\n",
       "      <td>freaky</td>\n",
       "      <td>slashing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfortunate</th>\n",
       "      <td>regrettable</td>\n",
       "      <td>obvious</td>\n",
       "      <td>unforgivable</td>\n",
       "      <td>loathed</td>\n",
       "      <td>weakness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictable</th>\n",
       "      <td>implausible</td>\n",
       "      <td>unrealistic</td>\n",
       "      <td>uninvolving</td>\n",
       "      <td>contrived</td>\n",
       "      <td>cliched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hilarious</th>\n",
       "      <td>funny</td>\n",
       "      <td>hysterical</td>\n",
       "      <td>hillarious</td>\n",
       "      <td>comical</td>\n",
       "      <td>quotable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fascinating</th>\n",
       "      <td>facinating</td>\n",
       "      <td>sobering</td>\n",
       "      <td>engrossing</td>\n",
       "      <td>enthralling</td>\n",
       "      <td>compelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boring</th>\n",
       "      <td>pointless</td>\n",
       "      <td>uneventful</td>\n",
       "      <td>predictable</td>\n",
       "      <td>dull</td>\n",
       "      <td>uninteresting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confused</th>\n",
       "      <td>annoyed</td>\n",
       "      <td>bored</td>\n",
       "      <td>disturbed</td>\n",
       "      <td>dazed</td>\n",
       "      <td>distracted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sensitive</th>\n",
       "      <td>gentle</td>\n",
       "      <td>abrasive</td>\n",
       "      <td>oily</td>\n",
       "      <td>harsh</td>\n",
       "      <td>delicate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imaginative</th>\n",
       "      <td>entertaining</td>\n",
       "      <td>engaging</td>\n",
       "      <td>clever</td>\n",
       "      <td>exciting</td>\n",
       "      <td>compelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senseless</th>\n",
       "      <td>pointless</td>\n",
       "      <td>nonsensical</td>\n",
       "      <td>idiotic</td>\n",
       "      <td>absurd</td>\n",
       "      <td>meaningless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bland</th>\n",
       "      <td>unoriginal</td>\n",
       "      <td>uninspired</td>\n",
       "      <td>formulaic</td>\n",
       "      <td>lifeless</td>\n",
       "      <td>trite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointing</th>\n",
       "      <td>dissapointing</td>\n",
       "      <td>disapointing</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>diappointed</td>\n",
       "      <td>disapointed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0             1             2            3  \\\n",
       "broken               cracked      crumbled        dented      damaged   \n",
       "damaged               dented     defective        broken    scratched   \n",
       "awesome               awsome       amazing     fantastic     excelent   \n",
       "useful               usefull       helpful        doable     valuable   \n",
       "good                   great        decent            ok         nice   \n",
       "easy               difficult        simple         quick     dificult   \n",
       "violent               brutal      sadistic         cruel   disturbing   \n",
       "romantic           screwball       romance        comedy    fairytale   \n",
       "nasty                  gross        filthy          icky       freaky   \n",
       "unfortunate      regrettable       obvious  unforgivable      loathed   \n",
       "predictable      implausible   unrealistic   uninvolving    contrived   \n",
       "hilarious              funny    hysterical    hillarious      comical   \n",
       "fascinating       facinating      sobering    engrossing  enthralling   \n",
       "boring             pointless    uneventful   predictable         dull   \n",
       "confused             annoyed         bored     disturbed        dazed   \n",
       "sensitive             gentle      abrasive          oily        harsh   \n",
       "imaginative     entertaining      engaging        clever     exciting   \n",
       "senseless          pointless   nonsensical       idiotic       absurd   \n",
       "bland             unoriginal    uninspired     formulaic     lifeless   \n",
       "disappointing  dissapointing  disapointing  disappointed  diappointed   \n",
       "\n",
       "                           4  \n",
       "broken               chipped  \n",
       "damaged              openned  \n",
       "awesome           phenominal  \n",
       "useful             practical  \n",
       "good                     bad  \n",
       "easy                   cinch  \n",
       "violent         misogynistic  \n",
       "romantic              quirky  \n",
       "nasty               slashing  \n",
       "unfortunate         weakness  \n",
       "predictable          cliched  \n",
       "hilarious           quotable  \n",
       "fascinating       compelling  \n",
       "boring         uninteresting  \n",
       "confused          distracted  \n",
       "sensitive           delicate  \n",
       "imaginative       compelling  \n",
       "senseless        meaningless  \n",
       "bland                  trite  \n",
       "disappointing    disapointed  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [item[0] for item in word_vectors.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['broken','damaged','awesome','useful','good','easy','violent', 'romantic', 'nasty', 'unfortunate', \n",
    "                                      'predictable', 'hilarious', 'fascinating', 'boring','confused', 'sensitive',\n",
    "                                     'imaginative','senseless', 'bland','disappointing']}\n",
    "pd.DataFrame(similar_words).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Sentiment Analyser on the Amazon Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from cnn_docmodel import DocClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size = 185000 , test Size = 15000\n"
     ]
    }
   ],
   "source": [
    "train_corpus_amazon = corpus_amazon[:185000]\n",
    "train_target_amazon = target_amazon[:185000]\n",
    "\n",
    "test_corpus_amazon = corpus_amazon[185000:]\n",
    "test_target_amazon = target_amazon[185000:]\n",
    "\n",
    "print(\"Train Size = {} , test Size = {}\".format(len(train_corpus_amazon)\n",
    "                                                , len(test_corpus_amazon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_corpus = [wpt.tokenize(doc.lower()) for doc in train_corpus_amazon]\n",
    "tokenized_test_corpus = [wpt.tokenize(doc.lower()) for doc in test_corpus_amazon] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using zero padding to make document of same size. \n",
    "Also for OOV(out of vocabulary words) we will use index vocab_size+1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordIndex(word):\n",
    "    try :\n",
    "        return word_vectors.get_index(word) + 1\n",
    "    except:\n",
    "        return vocab_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_to_seq = [ [getWordIndex(token) for token in tokens] for tokens in tokenized_train_corpus]\n",
    "MAX_TR_SEQ_LEN = int(np.mean([len(seq) for seq in corpus_to_seq]))\n",
    "corpus_to_seq = pad_sequences(corpus_to_seq,MAX_TR_SEQ_LEN, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5002,   94,  645,    1,   79,   24,    2, 4195,  961,    3,    2,\n",
       "       5002,   11,   94,  645,   15,   28, 4239,    1,  101,    3,    2,\n",
       "         24,  394, 2315, 3751,   35,   18,   27,    7, 7510,    9,    7,\n",
       "        842, 2597, 7393,    1,   20,    2,  182,    6,  516,   44, 4186,\n",
       "          1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_to_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus_amazon_seq = [ [getWordIndex(token) for token in tokens] for tokens in tokenized_test_corpus]\n",
    "test_corpus_amazon_seq = pad_sequences(test_corpus_amazon_seq,MAX_TR_SEQ_LEN, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create CNN Document Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sentiment_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cnn_document_model (Documen  multiple                 2764231   \n",
      " tEncoder)                                                       \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             multiple                  257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,764,488\n",
      "Trainable params: 2,764,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier = DocClassifier(vocab_size+2, #0 for null tok & for OOV - vocab_size+1\n",
    "                           embedding_dim = 50,\n",
    "                           dropout_rate = 0.3,\n",
    "                           training = True)        \n",
    "classifier(np.array([np.arange(50)]))\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn_document_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     multiple                  2733450   \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           multiple                  2525      \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          multiple                  3775      \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          multiple                  5025      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         multiple                  0 (unused)\n",
      "                                                                 \n",
      " dense_6 (Dense)             multiple                  19456     \n",
      "                                                                 \n",
      " global_max_pooling1d_3 (Glo  multiple                 0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,764,231\n",
      "Trainable params: 2,764,231\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.doc_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inilialize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.concatenate([np.zeros((1,50)),\n",
    "                             word_vectors.vectors,\n",
    "                             np.expand_dims(np.mean(word_vectors.vectors, axis=0), axis=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.doc_model.embedding.set_weights([embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate SA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "#mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean()\n",
    "test_acc = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 0 Loss=0.7005 Acc=0.5000\n",
      "epoch 0 step 100 Loss=0.5983 Acc=0.6767\n",
      "epoch 0 step 200 Loss=0.4844 Acc=0.7576\n",
      "epoch 0 step 300 Loss=0.4321 Acc=0.7904\n",
      "epoch 0 step 400 Loss=0.3976 Acc=0.8118\n",
      "epoch 0 step 500 Loss=0.3758 Acc=0.8253\n",
      "epoch 0 step 600 Loss=0.3600 Acc=0.8352\n",
      "epoch 0 step 700 Loss=0.3485 Acc=0.8421\n",
      "epoch 0 step 800 Loss=0.3369 Acc=0.8484\n",
      "epoch 0 step 900 Loss=0.3294 Acc=0.8530\n",
      "epoch 0 step 1000 Loss=0.3231 Acc=0.8567\n",
      "epoch 0 step 1100 Loss=0.3171 Acc=0.8598\n",
      "epoch 0 step 1200 Loss=0.3120 Acc=0.8627\n",
      "epoch 0 step 1300 Loss=0.3072 Acc=0.8656\n",
      "epoch 0 step 1400 Loss=0.3017 Acc=0.8684\n",
      "epoch 0 step 1500 Loss=0.2978 Acc=0.8706\n",
      "epoch 0 step 1600 Loss=0.2947 Acc=0.8721\n",
      "epoch 0 step 1700 Loss=0.2917 Acc=0.8738\n",
      "epoch 0 step 1800 Loss=0.2887 Acc=0.8754\n",
      "epoch 0 step 1900 Loss=0.2860 Acc=0.8767\n",
      "epoch 0 step 2000 Loss=0.2839 Acc=0.8778\n",
      "epoch 0 step 2100 Loss=0.2812 Acc=0.8792\n",
      "epoch 0 step 2200 Loss=0.2793 Acc=0.8802\n",
      "epoch 0 step 2300 Loss=0.2770 Acc=0.8814\n",
      "epoch 0 step 2400 Loss=0.2749 Acc=0.8826\n",
      "epoch 0 step 2500 Loss=0.2727 Acc=0.8838\n",
      "epoch 0 step 2600 Loss=0.2708 Acc=0.8847\n",
      "epoch 0 step 2700 Loss=0.2693 Acc=0.8854\n",
      "epoch 0 step 2800 Loss=0.2679 Acc=0.8862\n",
      "Validation Loss = 0.2185 Acc=0.9135\n",
      "epoch 1 step 0 Loss=0.2040 Acc=0.9219\n",
      "epoch 1 step 100 Loss=0.2214 Acc=0.9092\n",
      "epoch 1 step 200 Loss=0.2148 Acc=0.9129\n",
      "epoch 1 step 300 Loss=0.2056 Acc=0.9176\n",
      "epoch 1 step 400 Loss=0.1990 Acc=0.9213\n",
      "epoch 1 step 500 Loss=0.1957 Acc=0.9226\n",
      "epoch 1 step 600 Loss=0.1945 Acc=0.9237\n",
      "epoch 1 step 700 Loss=0.1932 Acc=0.9246\n",
      "epoch 1 step 800 Loss=0.1922 Acc=0.9247\n",
      "epoch 1 step 900 Loss=0.1918 Acc=0.9250\n",
      "epoch 1 step 1000 Loss=0.1910 Acc=0.9253\n",
      "epoch 1 step 1100 Loss=0.1901 Acc=0.9254\n",
      "epoch 1 step 1200 Loss=0.1889 Acc=0.9262\n",
      "epoch 1 step 1300 Loss=0.1876 Acc=0.9268\n",
      "epoch 1 step 1400 Loss=0.1855 Acc=0.9278\n",
      "epoch 1 step 1500 Loss=0.1840 Acc=0.9285\n",
      "epoch 1 step 1600 Loss=0.1834 Acc=0.9289\n",
      "epoch 1 step 1700 Loss=0.1834 Acc=0.9288\n",
      "epoch 1 step 1800 Loss=0.1830 Acc=0.9289\n",
      "epoch 1 step 1900 Loss=0.1822 Acc=0.9294\n",
      "epoch 1 step 2000 Loss=0.1820 Acc=0.9295\n",
      "epoch 1 step 2100 Loss=0.1816 Acc=0.9297\n",
      "epoch 1 step 2200 Loss=0.1810 Acc=0.9300\n",
      "epoch 1 step 2300 Loss=0.1804 Acc=0.9302\n",
      "epoch 1 step 2400 Loss=0.1794 Acc=0.9307\n",
      "epoch 1 step 2500 Loss=0.1786 Acc=0.9311\n",
      "epoch 1 step 2600 Loss=0.1778 Acc=0.9315\n",
      "epoch 1 step 2700 Loss=0.1775 Acc=0.9318\n",
      "epoch 1 step 2800 Loss=0.1773 Acc=0.9319\n",
      "Validation Loss = 0.2192 Acc=0.9153\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((corpus_to_seq,train_target_amazon))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "    \n",
    "    test_dataset_iterator = tf.data.Dataset.from_tensor_slices((test_corpus_amazon_seq,test_target_amazon))\n",
    "    test_dataset_iterator = test_dataset_iterator.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "    loss_metric.reset_states()\n",
    "    acc.reset_states()\n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    test_acc.reset_states()\n",
    "    \n",
    "    for step, (documents_batch, target_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred =  classifier(documents_batch)\n",
    "            loss = tf.reduce_mean(cross_entropy(target_batch, pred))\n",
    "        gradients = tape.gradient(loss, classifier.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, classifier.trainable_weights))  \n",
    "        loss_metric(loss)\n",
    "        acc.update_state(target_batch, pred)\n",
    "        if step % 100==0:\n",
    "            print(\"epoch %d step %d Loss=%.4f Acc=%.4f\" % (epoch,\n",
    "                                                  step, \n",
    "                                                  loss_metric.result(),\n",
    "                                                  acc.result()))\n",
    "\n",
    "            \n",
    "    for test_step, (test_docs, test_targets) in enumerate(test_dataset_iterator) :\n",
    "        test_pred =  classifier(test_docs)\n",
    "        testloss = tf.reduce_mean(cross_entropy(test_targets, test_pred))\n",
    "        test_loss(testloss)\n",
    "        test_acc.update_state(test_targets, test_pred)\n",
    "    print(\"Validation Loss = %.4f Acc=%.4f\"% (test_loss.result(), test_acc.result()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier.save_weights('./model/sa/sentiment_analyser')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on IMDB test w/o Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus_imdb = test_df_imdb['review'].tolist()\n",
    "test_target_imdb = test_df_imdb['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_corpus_imdb = [wpt.tokenize(doc.lower()) for doc in test_corpus_imdb]\n",
    "test_corpus_imdb_seq = [ [getWordIndex(token) for token in tokens] for tokens in tokenized_test_corpus_imdb]\n",
    "test_corpus_imdb_seq = pad_sequences(test_corpus_imdb_seq,MAX_TR_SEQ_LEN, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_imdb_iterator = tf.data.Dataset.from_tensor_slices((test_corpus_imdb_seq,test_target_imdb))\n",
    "test_dataset_imdb_iterator = test_dataset_imdb_iterator.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x289aeaa7e48>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_imdb = DocClassifier(vocab_size+2,\n",
    "                           embedding_dim = 50,\n",
    "                           dropout_rate = 0.3,\n",
    "                           training = True)        \n",
    "classifier_imdb(np.array([np.arange(50)]))\n",
    "#inilialize    \n",
    "classifier_imdb.load_weights('./model/sa/sentiment_analyser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.4726 Acc=0.8079\n"
     ]
    }
   ],
   "source": [
    "test_loss = tf.keras.metrics.Mean()\n",
    "test_acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "for test_step, (test_docs, test_targets) in enumerate(test_dataset_imdb_iterator) :\n",
    "    test_pred =  classifier_imdb(test_docs)\n",
    "    testloss = tf.reduce_mean(cross_entropy(test_targets, test_pred))\n",
    "    test_loss(testloss)\n",
    "    test_acc.update_state(test_targets, test_pred)\n",
    "print(\"Test Loss = %.4f Acc=%.4f\"% (test_loss.result(), test_acc.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_init=np.zeros_like(classifier_imdb.fc1.get_weights()[0])\n",
    "zero_init_bias=np.zeros_like(classifier_imdb.fc1.get_weights()[1])\n",
    "classifier_imdb.fc1.set_weights([zero_init, zero_init_bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_imdb_small = train_df_imdb.sample(frac=0.30, random_state = 42)\n",
    "corpus_imdb_small = train_df_imdb_small['review'].tolist()\n",
    "target_imdb_small = train_df_imdb_small['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_imdb_small_train_corpus = [wpt.tokenize(doc.lower()) for doc in corpus_imdb_small]\n",
    "corpus_imdb_small_seq = [ [getWordIndex(token) for token in tokens] for tokens in tokenized_imdb_small_train_corpus]\n",
    "corpus_imdb_small_seq = pad_sequences(corpus_imdb_small_seq,MAX_TR_SEQ_LEN, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean()\n",
    "test_acc = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 0 Loss=0.6931 Acc=0.5625\n",
      "epoch 0 step 100 Loss=0.6327 Acc=0.8054\n",
      "epoch 0 step 200 Loss=0.5683 Acc=0.8113\n",
      "Test Loss = 0.4417 Acc=0.8121\n",
      "epoch 1 step 0 Loss=0.5509 Acc=0.7188\n",
      "epoch 1 step 100 Loss=0.4193 Acc=0.8236\n",
      "epoch 1 step 200 Loss=0.4095 Acc=0.8266\n",
      "Test Loss = 0.3986 Acc=0.8175\n",
      "epoch 2 step 0 Loss=0.5169 Acc=0.7500\n",
      "epoch 2 step 100 Loss=0.3683 Acc=0.8394\n",
      "epoch 2 step 200 Loss=0.3739 Acc=0.8344\n",
      "Test Loss = 0.3920 Acc=0.8190\n",
      "epoch 3 step 0 Loss=0.3548 Acc=0.8125\n",
      "epoch 3 step 100 Loss=0.3551 Acc=0.8419\n",
      "epoch 3 step 200 Loss=0.3519 Acc=0.8456\n",
      "Test Loss = 0.3889 Acc=0.8210\n",
      "epoch 4 step 0 Loss=0.2871 Acc=0.8750\n",
      "epoch 4 step 100 Loss=0.3323 Acc=0.8546\n",
      "epoch 4 step 200 Loss=0.3322 Acc=0.8576\n",
      "Test Loss = 0.3876 Acc=0.8221\n",
      "epoch 5 step 0 Loss=0.2787 Acc=0.8438\n",
      "epoch 5 step 100 Loss=0.3157 Acc=0.8660\n",
      "epoch 5 step 200 Loss=0.3138 Acc=0.8666\n",
      "Test Loss = 0.3928 Acc=0.8222\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 6\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((corpus_imdb_small_seq,target_imdb_small))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)\n",
    "    \n",
    "    test_dataset_iterator = tf.data.Dataset.from_tensor_slices((test_corpus_imdb_seq,test_target_imdb))\n",
    "    test_dataset_iterator = test_dataset_iterator.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "    loss_metric.reset_states()\n",
    "    acc.reset_states()\n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    test_acc.reset_states()\n",
    "    \n",
    "    for step, (documents_batch, target_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred =  classifier_imdb(documents_batch)\n",
    "            loss = tf.reduce_mean(cross_entropy(target_batch, pred))\n",
    "        final_layer_weights = classifier_imdb.trainable_weights\n",
    "        gradients = tape.gradient(loss, final_layer_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, final_layer_weights))  \n",
    "        loss_metric(loss)\n",
    "        acc.update_state(target_batch, pred)\n",
    "        if step % 100==0:\n",
    "            print(\"epoch %d step %d Loss=%.4f Acc=%.4f\" % (epoch,\n",
    "                                                  step, \n",
    "                                                  loss_metric.result(),\n",
    "                                                  acc.result()))\n",
    "\n",
    "    for test_step, (test_docs, test_targets) in enumerate(test_dataset_imdb_iterator) :\n",
    "        test_pred =  classifier_imdb(test_docs)\n",
    "        testloss = tf.reduce_mean(cross_entropy(test_targets, test_pred))\n",
    "        test_loss(testloss)\n",
    "        test_acc.update_state(test_targets, test_pred)\n",
    "    print(\"Test Loss = %.4f Acc=%.4f\"% (test_loss.result(), test_acc.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on IMDB From Scrach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_imdb = DocClassifier(vocab_size+2,\n",
    "                           embedding_dim = 50,\n",
    "                           dropout_rate = 0.3,\n",
    "                           training = True)        \n",
    "classifier_imdb(np.array([np.arange(50)]))\n",
    "#inilialize embeddings   \n",
    "classifier_imdb.doc_model.embedding.set_weights([embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean()\n",
    "test_acc = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 0 Loss=0.7441 Acc=0.4375\n",
      "epoch 0 step 100 Loss=0.6928 Acc=0.5213\n",
      "epoch 0 step 200 Loss=0.6902 Acc=0.5350\n",
      "Test Loss = 0.6836 Acc=0.5836\n",
      "epoch 1 step 0 Loss=0.6856 Acc=0.5000\n",
      "epoch 1 step 100 Loss=0.6819 Acc=0.5727\n",
      "epoch 1 step 200 Loss=0.6814 Acc=0.5766\n",
      "Test Loss = 0.6785 Acc=0.6022\n",
      "epoch 2 step 0 Loss=0.6759 Acc=0.6562\n",
      "epoch 2 step 100 Loss=0.6755 Acc=0.6061\n",
      "epoch 2 step 200 Loss=0.6754 Acc=0.6007\n",
      "Test Loss = 0.6739 Acc=0.6135\n",
      "epoch 3 step 0 Loss=0.6661 Acc=0.6562\n",
      "epoch 3 step 100 Loss=0.6715 Acc=0.6194\n",
      "epoch 3 step 200 Loss=0.6708 Acc=0.6182\n",
      "Test Loss = 0.6700 Acc=0.6207\n",
      "epoch 4 step 0 Loss=0.6999 Acc=0.4375\n",
      "epoch 4 step 100 Loss=0.6663 Acc=0.6071\n",
      "epoch 4 step 200 Loss=0.6670 Acc=0.6096\n",
      "Test Loss = 0.6689 Acc=0.5858\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((corpus_imdb_small_seq,target_imdb_small))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)\n",
    "    \n",
    "    test_dataset_iterator = tf.data.Dataset.from_tensor_slices((test_corpus_imdb_seq,test_target_imdb))\n",
    "    test_dataset_iterator = test_dataset_iterator.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "    loss_metric.reset_states()\n",
    "    acc.reset_states()\n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    test_acc.reset_states()\n",
    "    \n",
    "    for step, (documents_batch, target_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred =  classifier_imdb(documents_batch)\n",
    "            loss = tf.reduce_mean(cross_entropy(target_batch, pred))\n",
    "        final_layer_weights = classifier_imdb.fc1.trainable_weights + classifier_imdb.doc_model.doc_embedding.trainable_weights\n",
    "        gradients = tape.gradient(loss, final_layer_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, final_layer_weights))  \n",
    "        loss_metric(loss)\n",
    "        acc.update_state(target_batch, pred)\n",
    "        if step % 100==0:\n",
    "            print(\"epoch %d step %d Loss=%.4f Acc=%.4f\" % (epoch,\n",
    "                                                  step, \n",
    "                                                  loss_metric.result(),\n",
    "                                                  acc.result()))\n",
    "\n",
    "    for test_step, (test_docs, test_targets) in enumerate(test_dataset_imdb_iterator) :\n",
    "        test_pred =  classifier_imdb(test_docs)\n",
    "        testloss = tf.reduce_mean(cross_entropy(test_targets, test_pred))\n",
    "        test_loss(testloss)\n",
    "        test_acc.update_state(test_targets, test_pred)\n",
    "    print(\"Test Loss = %.4f Acc=%.4f\"% (test_loss.result(), test_acc.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
